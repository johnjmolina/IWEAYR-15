{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patheffects as patheffects\n",
    "import cmocean as cmo\n",
    "import glob\n",
    "\n",
    "from tqdm import tqdm\n",
    "from jax import grad, jit, vmap, jacfwd, jacrev, lax\n",
    "from jax.config import config; config.update(\"jax_enable_x64\", True)\n",
    "from jax import random\n",
    "from functools import partial\n",
    "from scipy import optimize\n",
    "from natsort import natsorted\n",
    "mpl.style.use(['seaborn-poster', 'seaborn-muted'])\n",
    "\n",
    "#betanalphas' colormap\n",
    "colors = [\"#DCBCBC\",\"#C79999\",\"#B97C7C\",\"#A25050\",\"#8F2727\", \"#7C0000\",\"#DCBCBC20\", \"#8F272720\",\"#00000060\"]\n",
    "color  = {i[0]:i[1] for i in zip(['light','light_highlight','mid','mid_highlight','dark','dark_highlight','light_trans','dark_trans','superfine'],colors)}\n",
    "fancycolors = [mpl.colors.to_hex(c) for c in [[0.6, 0.6, 0.6],[0.7, 0.3, 1],[0.3, 0.7, 1],[0.2, 0.9, 0.9],\n",
    "                                              [0.3, 1, 0.7],[0.7, 1, 0.3],[0.9, 0.9, 0.2],[1, 0.7, 0.3],[1, 0.3, 0.7],\n",
    "                                              [0.9, 0.2, 0.9],[1.0, 1.0, 1.0]]]\n",
    "threecolors = [mpl.colors.to_hex(c) for c in [[0.1, 0.15, 0.4],[1, 0.2, 0.25],[1.0, 0.775, 0.375]]]\n",
    "fourcolors = [mpl.colors.to_hex(c) for c in [[0.9, 0.6, 0.3],[0.9, 0.4, 0.45],[0.5, 0.65, 0.75],[0.42, 0.42, 0.75]]]\n",
    "\n",
    "def addtxt(ax, x, y, txt, fs=8, lw=3, clr='k', bclr='w', rot=0):\n",
    "    \"\"\"Add text to figure axis\"\"\"\n",
    "    return ax.text(x, y, txt, color=clr, ha='left', transform=ax.transAxes, rotation=rot, weight='bold',\n",
    "                   path_effects=[patheffects.withStroke(linewidth=lw, foreground=bclr)], fontsize=fs)\n",
    "def hessian(f):\n",
    "    \"\"\"Returns a function which computes the Hessian of a function f\n",
    "           if f(x) gives the values of the function at x, and J = hessian(f)\n",
    "           J(x) gives the Hessian at x\"\"\"\n",
    "    return jit(jacfwd(jacrev(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical convenience, we will work with the logarithm of the hyperparameters.\n",
    "For example, for the Square Exponential kernel we have\n",
    "\n",
    "\\begin{align}\n",
    "K(x_1,x_2) &= \\gamma \\exp{\\left[-\\frac{1}{2}\\frac{(x_1-x_2)^2}{l^2}\\right]} \\\\\n",
    "&= \\exp{\\left[\\log{\\gamma} - \\frac{1}{2} \\left(e^{-\\log{l}} (x_1-x_2)\\right)^2\\right]} \\\\\n",
    "&= \\exp{\\left[\\theta_0 - \\frac{1}{2}\\left(e^{-\\theta_1} (x_1 - x_2)\\right)^2\\right]} \\\\\n",
    "\\Theta &= (\\log{\\gamma}, \\log{l})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def K_OrnsteinUhlenbeck(x1, x2, Î¸):\n",
    "    logÎ³, logl = Î¸\n",
    "    return np.exp(logÎ³ - 0.5*np.exp(-logl)*np.abs(x1-x2))\n",
    "\n",
    "@jit\n",
    "def K_Matern32(x1,x2,Î¸):\n",
    "    logÎ³, logl = Î¸\n",
    "    r = np.sqrt(3.0)*np.abs(x1-x2)*np.exp(-logl)\n",
    "    return (1.0 + r)*np.exp(logÎ³-r)\n",
    "@jit\n",
    "def K_Matern52(x1,x2,Î¸):\n",
    "    logÎ³, logl = Î¸\n",
    "    r = np.sqrt(5.0)*np.abs(x1-x2)*np.exp(-logl)\n",
    "    return (1.0 + r + (1.0/3.0)*r**2.0)*np.exp(logÎ³-r)\n",
    "@jit\n",
    "def K_Matern72(x1,x2,Î¸):\n",
    "    logÎ³, logl = Î¸\n",
    "    r = np.sqrt(7.0)*np.abs(x1-x2)*np.exp(-logl)\n",
    "    return (1.0 + r + (2.0/5.0)*r**2.0 + (1.0/15.0)*r**3.0)*np.exp(logÎ³-r)\n",
    "@jit\n",
    "def K_SquareExp(x1,x2,Î¸):\n",
    "    logÎ³, logl = Î¸\n",
    "    return np.exp(logÎ³-0.5*((x1-x2)*np.exp(-logl))**2)\n",
    "@jit\n",
    "def K_NN(x0,x1,Î¸):\n",
    "    Ïƒ0,Ïƒ = np.exp(Î¸)\n",
    "    return 2.0/np.pi*np.arcsin(2.0*(Ïƒ0 + Ïƒ*x0*x1)/np.sqrt((1.0 + 2.0*(Ïƒ0 + Ïƒ*x0**2.0)) * (1.0 + 2.0*(Ïƒ0 + Ïƒ*x1**2))))\n",
    "\n",
    "@jit \n",
    "def K_RQ(x1,x2,Î¸):\n",
    "    Î³, l, Î± = np.exp(2*Î¸)\n",
    "    return Î³*(1 + ((((x1-x2)/l)**2)/(2*Î±)))**(-Î±)\n",
    "\n",
    "# broadcast a function of three variables variables f(a,b,Î¸), such that for x = [x1,...,xn] and y = [y1,...,ym]\n",
    "# outermap(f)(x,y,Î¸) = [[f(x_1,y_1,Î¸), f(x_1,y_2,Î¸), ... , f(x_1,y_m,Î¸)],\n",
    "#                       [f(x_2,y_1,Î¸), f(x_2,y_2,Î¸), ... , f(x_2,y_m,Î¸)],\n",
    "#                       ...\n",
    "#                       [f(x_n,y_1,Î¸), f(x_n,y_2,Î¸), ... , f(x_n,y_m,Î¸)]] \n",
    "outermap = lambda f : vmap(vmap(f, in_axes=(None,0,None)), in_axes=(0,None,None)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def logpGP(Î´f, Î£, Ïµ):\n",
    "    \"\"\"Compute minus log-likelihood of observing Î´f = f - <f>, for GP with covariance matrix Î£\"\"\"\n",
    "    n     = len(Î´f)\n",
    "    noise = np.ones_like(Î´f)*Ïµ   # jiggle parameter to improve numerical stability of cholesky decomposition\n",
    "    L     = np.linalg.cholesky(Î£ + np.diag(noise))\n",
    "    v     = np.linalg.solve(L, Î´f)\n",
    "    return (0.5*np.dot(v, v) + np.sum(np.log(np.diag(L))) + 0.5*n*np.log(2.0*np.pi))\n",
    "\n",
    "@jit\n",
    "def postGP(Î´fb, Kaa, Kab, Kbb):\n",
    "    \"\"\"Compute posterior average and covariance from conditional GP p(fa | xa, xb, fb)\n",
    "    [fa,fb] ~ ð’©([Î¼_fa, Î¼_fb], [[Kaa, Kab],[Kab^T, Kbb]])])\n",
    "    fa|fb   ~ ð’©(Î¼f + Kab Kbb \\ (fb - Î¼_fb) , Kaa - Kab Kbb \\ Kab^T)\n",
    "    \"\"\"\n",
    "    L = np.linalg.cholesky(Kbb)\n",
    "    \n",
    "    # Î± = K \\ Î´ f = L^t \\ (L | Î´ f)\n",
    "    Î± = np.linalg.solve(L.transpose(), np.linalg.solve(L, Î´fb))\n",
    "    \n",
    "    # Î¼post - Î¼(x*) = Kab Kbb \\ Î´f(x) = Kab . Î±\n",
    "    Î¼post = np.dot(Kab, Î±)\n",
    "\n",
    "    # Kpost = Kaa - Kab Kbb | Kab^T \n",
    "    #       = Kaa - W\n",
    "    # W_ij  = v_i . v_j \n",
    "    # v_i   = (L | c_i) ; c_i the i-th column of Kba, i-th row of Kab\n",
    "    V     = np.array([np.linalg.solve(L, c) for c in Kab]) # V = [v_1, v_2, ... ]^t\n",
    "    Kpost = Kaa - np.einsum('ik,jk->ij',V,V)\n",
    "    return Î¼post, Kpost # note should add Î¼(x*) to average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to infer the steady state 1D Stokes flow\n",
    "\\begin{align}\n",
    "\\frac{\\text{d}^2 v}{\\text{d} x^2} &= v_{xx} =  \\Delta \\widetilde{P} \\Longrightarrow\n",
    "\\mathcal{L}_x v = \\Delta\\widetilde{P}\\\\\n",
    "\\mathcal{L} &\\equiv \\frac{\\text{d}^2}{\\text{d}x^2}\\\\\n",
    "\\Delta \\widetilde{P} &= \\frac{\\Delta P}{\\eta L}\n",
    "\\end{align}\n",
    "\n",
    "with boundary conditions $v(x=0) = v_0$ and $v(x=1) = v_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placing a GP prior on $v$, the linearity of the Stokes equation leads to the following GP for the joint distribution of the velocity and its second derivatives\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}v^\\star \\\\ v \\\\ v_{xx}\\end{pmatrix} &=\n",
    "\\mathcal{N}\\left(\\begin{bmatrix}\\mu_{v^\\star} \\\\ \\mu_{v} \\\\ \\mu_{v_{xx}}\\end{bmatrix}, \n",
    "\\begin{bmatrix} \n",
    "K^{v^\\star, v^\\star} & K^{v^\\star, v} & K^{v^\\star, v_{xx}} \\\\\n",
    "& K^{v, v} & K^{v, v_{xx}} \\\\\n",
    "& & K^{v_{xx}, v_{xx}}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As training points we have the values of the velocity $v$ and the second derivatives $\\partial_{xx} v$ at some arbitrary positions, not necessarily the same for both. In particular, we will be interested in the case where the training points for the velocity are specified at the domain boundaries.\n",
    "\n",
    "Given the linearity of the laplacian, the covariance Kernel can be written solely in terms of $K^{vv} = K$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\begin{pmatrix}v^\\star \\\\ v \\\\ v_{xx}\\end{pmatrix} &=\n",
    "\\mathcal{N}\\left(\\begin{bmatrix}\\mu_{v^\\star} \\\\ \\mu_{v} \\\\ \\mu_{v_{xx}}\\end{bmatrix}, \n",
    "\\begin{bmatrix} \n",
    "K(x^\\star, x^\\star) & K(x^\\star, x) & \\mathcal{L}_2 K(x^\\star, x^f) \\\\\n",
    "& K(x, x) & \\mathcal{L}_2 K(x, x^f) \\\\\n",
    "& & \\mathcal{L}_1\\mathcal{L}_2 K(x^f, x^f)\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "\\end{align}\n",
    "\n",
    "where $x^\\star$, $x$, and $x^f$ are the locations of the velocity test points, velocity training points, and second derivative training points, respectively, and the subscript in $\\mathcal{L}$ refers to which of the two arguments of the kernel function it is acting on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=(4,5))\n",
    "def mixedK(Î¸, x0, x1, x2, K01, K02):\n",
    "    \"\"\"Compute off diagonal block matrix [K(x*, x), L_2 K(x*, x')]\n",
    "    Args :\n",
    "      Î¸  : kernel hyperparamters\n",
    "      x0 : test points for velocity\n",
    "      x1 : training points for velocity (i.e., boundary points)\n",
    "      x2 : trainnig points for forces\n",
    "      K01: velocity/velocity covariance function = K_vv\n",
    "      K02: velocity/force covariance function = L_2 K_vv\n",
    "    Returns:\n",
    "      off diagonal covariance matrix\n",
    "    \"\"\"\n",
    "    n0,n1,n2 = len(x0), len(x1), len(x2)\n",
    "    Î£ = np.zeros((n0,n1+n2))\n",
    "    Î£ = Î£.at[:,:n1].set(K01(x0, x1, Î¸))\n",
    "    Î£ = Î£.at[:,n1:].set(K02(x0, x2, Î¸))\n",
    "    return Î£\n",
    "    \n",
    "@partial(jit, static_argnums=(3,4,5))\n",
    "def trainingK(Î¸, x1, x2, K11, K12, K22): \n",
    "    \"\"\"Compute training block matrix \n",
    "    [[K(x,x)         L_2 K(x, xf)],         ->   [[ K11(x1,x1)    K12(x1, x2)\n",
    "     [L_1 K(xf, x)   L_1 L_2 K(xf, xf)]]          [ K12(x1,x2)^T  K22(x2,x2)]\n",
    "    Args :\n",
    "      Î¸  : kernel hyperparameters\n",
    "      x1 : training points for velocity (i.e., boundary points)\n",
    "      x2 : training points for forces\n",
    "      K11: velocity/velocity covariance function = K_vv\n",
    "      K12: velocity/force covaraince function = L K_vv\n",
    "      K22: force/force covariance function = L L K_vv\n",
    "    Returns:\n",
    "      training covariance matrix\n",
    "    \"\"\"\n",
    "    n1,n2 = len(x1), len(x2)\n",
    "    Î£ = np.zeros((n1+n2,n1+n2))\n",
    "    Î£ = Î£.at[:n1, :n1].set(K11(x1, x1, Î¸))\n",
    "    Î£ = Î£.at[:n1, n1:].set(K12(x1, x2, Î¸))\n",
    "    Î£ = Î£.at[n1:, n1:].set(K22(x2, x2, Î¸))\n",
    "    Î£ = Î£.at[n1:, :n1].set(np.transpose(Î£[:n1, n1:]))\n",
    "    return Î£\n",
    "\n",
    "def flowGP(Kernel):\n",
    "    \"\"\"Generate functions used to compute log-likelihood (to be minimized) and prediction functions \n",
    "    Args :\n",
    "      Kernel  : kernel function to use (RBF, NN, Matern, etc..), should be three paramters function k(x,x',Î¸) \"\"\"\n",
    "    L    = lambda f, i : grad(grad(f, i), i) # L = d^2 / dx^2\n",
    "    _LK  = L(Kernel, 1)                      # (L_2 K)(x*,x')\n",
    "    _LLK = L(_LK, 0)                         # (L_1 L_2 K)(x',x')\n",
    "\n",
    "    K    = jit(outermap(Kernel))\n",
    "    LK   = jit(outermap(_LK))\n",
    "    LLK  = jit(outermap(_LLK))\n",
    "\n",
    "    def trainingFunction(Î¸, *args):\n",
    "        \"\"\"Returns minus log-likelihood given Kernel hyperparamters Î¸ and training data args\n",
    "        args = velocity position, velocity average, velocity values, \n",
    "               force position, force average, force values, \n",
    "               jiggle parameter\n",
    "        \"\"\"\n",
    "        xv, Î¼v, yv, xf, Î¼f, yf, Ïµ = args        \n",
    "        Î´y = np.concatenate([yv - Î¼v, yf - Î¼f]) # create single training array, with velocities and forces (second derivatives)\n",
    "        Î£ = trainingK(Î¸, xv, xf, K, LK, LLK)\n",
    "        return logpGP(Î´y, Î£, Ïµ)\n",
    "    \n",
    "    def predictingFunction(Î¸, *args):\n",
    "        \"\"\"Returns conditional posterior average and covariance matrix given Kernel hyperparamters Î¸  and test and training data\n",
    "        args = test velocity position, test velocity average,\n",
    "               training velocity position, training velocity average, training velocity values\n",
    "               training force position, training force average, training force values\n",
    "               jiggle parameter\n",
    "        \"\"\"\n",
    "        xp, Î¼p, xv, Î¼v, yv, xf, Î¼f, yf, Ïµ = args\n",
    "        nb  = len(xv) + len(xf)   # total number of training data points (velocity + forces) \n",
    "        Î£bb = trainingK(Î¸, xv, xf, K, LK, LLK) + np.diag(np.ones(nb)*Ïµ)\n",
    "        Î£ab = mixedK(Î¸, xp, xv, xf, K, LK)\n",
    "        Î£aa = K(xp, xp, Î¸)\n",
    "        Î¼post,Î£post = postGP(np.concatenate([yv-Î¼v, yf-Î¼f]), Î£aa, Î£ab, Î£bb)\n",
    "        return Î¼p + Î¼post, Î£post\n",
    "    \n",
    "    return jit(trainingFunction), jit(predictingFunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeParameters(f, df, hf, init, *args):\n",
    "    def printer(o):\n",
    "        print(f'\\t loss = {o[\"fun\"]:12.6e}, niter = {o[\"nit\"]:5d}, Converged = {o[\"success\"]:6b} : {o[\"message\"]}')\n",
    "        print(f'\\t\\t     Î¸0 = {np.exp(init)}')        \n",
    "        print(f'\\t\\t      Î¸ = {np.exp(o[\"x\"])}')\n",
    "        print(f'\\t\\t log(Î¸) = {o[\"x\"]}')\n",
    "        \n",
    "    opt = {'maxiter':1000, 'disp':0}\n",
    "    res = [{'x':init}]\n",
    "    xv, _, _, xf, *_ = args\n",
    "    ntraining = len(xv) + len(xf)\n",
    "    func  = lambda p,*args : f(p, *args)/ntraining**2\n",
    "    dfunc = lambda p,*args : df(p,*args)/ntraining**2\n",
    "    hess  = lambda p,*args : hf(p,*args)/ntraining**2  \n",
    "    res.append(optimize.minimize(func, res[-1]['x'], args=args, method='Nelder-Mead', options=opt))\n",
    "    res.append(optimize.minimize(func, res[-1]['x'], args=args, method='TNC', jac=dfunc,options=opt))      \n",
    "    res.append(optimize.minimize(func, res[-1]['x'], args=args, method='L-BFGS-B', options=opt))\n",
    "    res.append(optimize.minimize(func, res[-1]['x'], args=args, method='BFGS', jac=dfunc,options=opt))  \n",
    "    \n",
    "    printer(res[-1])             \n",
    "    return res\n",
    "\n",
    "def solution(func, dfunc, hess, pred, init, *args):\n",
    "    opt = optimizeParameters(func, dfunc, hess, init, *args[2:])\n",
    "    Î¸0  = opt[-1]['x']\n",
    "    v,Î£ = pred(Î¸0, *args)\n",
    "    dv  = np.sqrt(np.diag(Î£))\n",
    "    print(f'\\t max(Ïƒ_v) = {np.max(dv):.1e}')\n",
    "    print(f'\\n')\n",
    "    return v, dv, opt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(*, \n",
    "                a_new_v, b_new_v, n_new_v,  \n",
    "                a_obs_f, b_obs_f, n_obs_f, Î”p,                \n",
    "                a_obs_v, b_obs_v, va_obs, vb_obs,                \n",
    "                fboundary,\n",
    "                Î´,\n",
    "                Ïµ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    a_new_v, b_new_v, n_new_v: [a,b] limits for the n_new test points at which velocity will be computed\n",
    "    a_obs_p, b_obs_p, n_obs_p, Î”p    : [a,b] limits for (inner) n_obs training points at which forces (velocity second derivatives = Î”p) are observed\n",
    "    a_obs_v, b_obs_v, va_obs, vb_obs : xa,xb, v(xa), v(xb), are the observed values of velocity at boundaries \n",
    "    fboundary : true|false, whether or not to include boundary points in force training data\n",
    "    Î´ : the offset by which to shrink the observation domain for the pressures, [a+Î´, b+Î´], so they don't necessarily fall on the boundary\n",
    "    Ïµ : the jiggle parameter to improve numerical stability of the Cholesky decomposition\n",
    "\n",
    "    Returns:\n",
    "        xnew_v, Î¼new_v, xobs_v, Î¼obs_v, yobs_v, xobs_f, Î¼obs_f, yobs_f, Ïµ**2\n",
    "    \"\"\"\n",
    "    if not fboundary:\n",
    "        xobs_f = np.linspace(a_obs_f + Î´, b_obs_f - Î´, num=n_obs_f + 2)[1:-1] # remove boundaries\n",
    "    else:\n",
    "        xobs_f = np.linspace(a_obs_f + Î´, b_obs_f - Î´, num=n_obs_f)\n",
    "\n",
    "    yobs_f  = np.ones_like(xobs_f)*Î”p/(b_obs_f-a_obs_f)\n",
    "\n",
    "    xobs_v = np.array([a_obs_v, b_obs_v])\n",
    "    yobs_v = np.array([va_obs, vb_obs])\n",
    "\n",
    "    xnew_v = np.linspace(a_new_v,b_new_v,num=n_new_v)\n",
    "    \n",
    "    # return argument list needed as input for GP functions\n",
    "    return (xnew_v, np.zeros_like(xnew_v),          # x, Î¼\n",
    "            xobs_v, np.zeros_like(yobs_v), yobs_v,  # x, Î¼, y\n",
    "            xobs_f, np.zeros_like(yobs_f), yobs_f,  # x, Î¼, y\n",
    "            Ïµ**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logposterior = lambda loglikelihood : (lambda Î¸, *args : loglikelihood(Î¸, *args) + np.sum(Î¸)) # log posterior = log likelihood + log prior (using Jeffrey's prior)\n",
    "\n",
    "loglikelihood_SE, predictor_SE = flowGP(K_SquareExp)\n",
    "func_SE  = jit(logposterior(loglikelihood_SE))\n",
    "dfunc_SE = jit(grad(func_SE))\n",
    "hess_SE  = hessian(func_SE)\n",
    "\n",
    "loglikelihood_RQ, predictor_RQ = flowGP(K_RQ)\n",
    "func_RQ  = jit(logposterior(loglikelihood_RQ))\n",
    "dfunc_RQ = jit(grad(func_RQ))\n",
    "hess_RQ  = hessian(func_RQ)\n",
    "\n",
    "loglikelihood_MT, predictor_MT, = flowGP(K_Matern52)\n",
    "func_MT  = jit(logposterior(loglikelihood_MT))\n",
    "dfunc_MT = jit(grad(func_MT))\n",
    "hess_MT  = hessian(func_MT)\n",
    "\n",
    "loglikelihood_NN, predictor_NN = flowGP(K_NN)\n",
    "func_NN  = jit(logposterior(loglikelihood_NN))\n",
    "dfunc_NN = jit(grad(func_NN))\n",
    "hess_NN  = hessian(func_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_solver(ftag,func, dfunc, hess, predictor,*,init,L, Î”p, n_obs, x_obs, v_obs, n_new, x_new,Î´,Ïµ):\n",
    "    solutions  = []\n",
    "    frc_points = []\n",
    "    opts       = [{'x':init}]\n",
    "    for nob in n_obs:\n",
    "        print(nob)\n",
    "        args  = initializer(a_new_v = x_new[0], b_new_v =x_new[1], n_new_v= n_new,\n",
    "                            a_obs_f = x_obs[0], b_obs_f =x_obs[1], n_obs_f= nob, Î”p = Î”p, \n",
    "                            a_obs_v = x_obs[0], b_obs_v =x_obs[1], va_obs = v_obs[0], vb_obs=v_obs[1], \n",
    "                            fboundary = False, Î´ = Î´, Ïµ = Ïµ)\n",
    "        frc_points.append(args[5])\n",
    "        ans = solution(func, dfunc, hess, predictor, opts[-1]['x'], *args)\n",
    "        if ans[2]['success']: # append solution if converged, use as initial guess for next iteration\n",
    "            opts.append(ans[-1])\n",
    "        solutions.append(ans)    \n",
    "\n",
    "        vtest_points , _, _, _, _, _ , *_  = args # velocity test and training points are always the same\n",
    "\n",
    "    df    = pd.DataFrame({'x':vtest_points}) # location of velocity test points\n",
    "    for i,num in enumerate(n_obs):\n",
    "        df[f'v_{num}'] = solutions[i][0]     # prediciton for velocity test points\n",
    "        df[f'dv_{num}']= solutions[i][1]     # uncertainty for velocity test points               \n",
    "        \n",
    "    \n",
    "    # Save data to file to avoid re-running calculations                 \n",
    "    for i,nob in enumerate(n_obs):\n",
    "        df_ft = pd.DataFrame({'x':frc_points[i]}) # location of force training points\n",
    "        df_ft.to_csv(f'./data/{ftag}_ft_n{nob}.txt', sep = ' ', float_format = \"%20.15e\")    \n",
    "    df.to_csv(f'./data/{ftag}.txt', sep = ' ', float_format = \"%20.15e\")\n",
    "\n",
    "    return {'train':frc_points, 'test':df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {'se':'Squared Exponential', 'rq':'Rational Quadratic', 'mt':'Matern', 'nn':'Neural Network'}\n",
    "# Load data from file\n",
    "def load_solution(ftag):\n",
    "    frc_points = []\n",
    "    for i,nob in enumerate(n_obs):\n",
    "        frc_points.append(np.array(pd.read_csv(f'./data/{ftag}_ft_n{nob}.txt', delim_whitespace=True)['x'].to_numpy()))\n",
    "\n",
    "    df = pd.read_csv(f'./data/{ftag}.txt', delim_whitespace=True)\n",
    "    return {'train':frc_points, 'test':df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savefig(fig, name, **kargs):\n",
    "    fig.savefig(f'./fig_stokes/{name}.png', **kargs)\n",
    "    fig.savefig(f'./fig_stokes/{name}.pdf', **kargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couette Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L,Î”p        = 1.0, 0.0, \n",
    "x_obs,v_obs = np.array([0.0, L]), np.array([-1.0, 1.0])\n",
    "n_obs       = [1, 2, 3, 4, 5, 6, 8, 16, 32, 64]         # number of force training/observation points in [0,L]\n",
    "\n",
    "x_new   = np.array([0.0, L])\n",
    "n_new   = 20\n",
    "couette = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "couette['se'] = scan_solver('couette_se', func_SE, dfunc_SE, hess_SE, predictor_SE, init = np.array([0.0, 1.0]),\n",
    "                            L=L, Î”p=Î”p, n_obs=n_obs, x_obs=x_obs, v_obs=v_obs, n_new=n_new, x_new=x_new, Î´=0.0, Ïµ=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "couette['mt'] = scan_solver('couette_mt', func_MT, dfunc_MT, hess_MT, predictor_MT, init = np.array([0.0, 0.0]),\n",
    "                            L=L, Î”p=Î”p, n_obs=n_obs, x_obs=x_obs, v_obs=v_obs, n_new=n_new, x_new=x_new, Î´=0.0, Ïµ=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "couette['rq'] = scan_solver('couette_rq', func_RQ, dfunc_RQ, hess_RQ, predictor_RQ, init = np.array([0.0, 0.0, 1.0]),\n",
    "                            L=L, Î”p=Î”p, n_obs=n_obs, x_obs=x_obs, v_obs=v_obs, n_new=n_new, x_new=x_new, Î´=0.0, Ïµ=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "couette['nn'] = scan_solver('couette_nn', func_NN, dfunc_NN, hess_NN, predictor_NN, init = np.array([0.0, 1.0]),\n",
    "                            L=L, Î”p=Î”p, n_obs=n_obs, x_obs=x_obs, v_obs=v_obs, n_new=n_new, x_new=x_new, Î´=0.0, Ïµ=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_solution(ans, name, inum, stag='se'):\n",
    "    fig, [ax,bx] = plt.subplots(figsize=(9,12),nrows=2, sharex=True)\n",
    "    ii = inum\n",
    "    ni = n_obs[ii]; print(f'(Force) Training Points = {ni}')\n",
    "\n",
    "    for lbl, clr in zip(ans.keys(), [color['dark_highlight'], color['mid_highlight'], color['light_highlight'], color['light']]):\n",
    "        df = ans[lbl]['test']\n",
    "        x, v, dv = df['x'].values, df[f'v_{ni}'].values, df[f'dv_{ni}'].values\n",
    "        ax.plot(x, v, color=clr, label=tags[lbl])\n",
    "        ax.fill_between(x, v-2*dv, v+2*dv, color=clr, alpha=0.8)\n",
    "        ax.plot(x, v, color=clr)\n",
    "\n",
    "    xf = ans['se']['train']\n",
    "    shift = (np.max(v)-np.min(v))*0.1\n",
    "    ax.plot(x_obs, v_obs, ls='None', marker='o', color=color['superfine'], label='Training Velocity')    \n",
    "    ax.plot(xf[ii], np.min(v)*np.ones_like(xf[ii]) - shift, ls='None', marker='s',mew = 3, mec=color['superfine'],\n",
    "            ms=12, label='Training Force Location', mfc='None')\n",
    "    ax.plot(x, vgold, ls='--', color=color['superfine'])\n",
    "    ax.set_ylabel(r'${v}$', fontsize=22)\n",
    "    ax.legend(loc=2, fontsize=15)\n",
    "\n",
    "    ### \n",
    "    lbl  = tags[stag]\n",
    "    df   = ans[stag]['test']\n",
    "    clrs = [color['light'], color['mid'], color['dark']]\n",
    "    dclrs= [color['light_highlight'], color['mid_highlight'], color['dark_highlight']]\n",
    "    for i,ni in enumerate(n_obs[:3]):\n",
    "        clr,dclr= clrs[i], dclrs[i]\n",
    "        v,dv = df[f'v_{ni}'].values, df[f'dv_{ni}'].values\n",
    "        bx.fill_between(x, v-2*dv, v+2*dv, color=clr, zorder=i,alpha=0.9, label=f'$n_f = {ni}$')\n",
    "        bx.plot(x, v, color=dclr, zorder=50+2*i, lw=1)#, label=f'$n_f = {ni}$')\n",
    "        bx.plot(xf[i], np.min(v)*np.ones_like(xf[i])-shift, ls='None', marker='s',mew = 4, mec=clr,\n",
    "                ms=16-5*i, mfc='None')\n",
    "    bx.plot(x_obs, v_obs, ls='None', marker='o', color=color['superfine'])    \n",
    "    bx.plot(x, vgold, ls='--', color=color['superfine'],zorder=50, lw=3)\n",
    "    bx.legend(fontsize=16)\n",
    "    bx.set_ylabel(r'${v}$', fontsize=22)\n",
    "    bx.set_xlabel(r'${x}$', fontsize=22)\n",
    "    addtxt(bx, 0.05, 0.92, lbl, fs=14)\n",
    "    savefig(fig, name, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload from disk\n",
    "couette = {tag : load_solution(f'couette_{tag}') for tag in tags.keys()}\n",
    "vgold   = (lambda x :(v_obs[1]-v_obs[0])/L*x + v_obs[0])(couette['se']['test']['x'].values)\n",
    "plot_solution(couette, 'couette', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error(ans, name):\n",
    "    fig, axes = plt.subplots(figsize=(9,8), ncols=2, nrows=2,sharex=True, sharey=True)\n",
    "    for ax,lbl in zip(axes.flatten(), ans.keys()):\n",
    "        df   = ans[lbl]['test']\n",
    "        clrs = cmo.cm.haline(np.linspace(0, 1, num=len(n_obs)))\n",
    "        for i, n in enumerate(n_obs):\n",
    "            x = df['x'].values\n",
    "            v = df[f'v_{n}'].values\n",
    "            dv= df[f'dv_{n}'].values\n",
    "            ax.plot(x,  np.abs(vgold - v), color=clrs[i])\n",
    "            addtxt(ax, 0.1, 0.05, tags[lbl], fs=15)\n",
    "    for ax in axes[1]:\n",
    "        ax.set_xlabel(r'${x}$', fontsize=22)\n",
    "    fig.suptitle('Absolute Error', fontsize=22)\n",
    "    ax.semilogy()\n",
    "    ax.set_ylim(1e-12,1e-0)\n",
    "    fig.tight_layout()\n",
    "    savefig(fig, 'name', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error(couette, 'couette_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "couette_epsilon = []\n",
    "tols  = [5e-6, 5e-5, 5e-4, 5e-3, 5e-2]\n",
    "tolbls = ['$5\\cdot 10^{-6}$', '$5\\cdot 10^{-5}$', '$5\\cdot 10^{-4}$', '$5\\cdot 10^{-3}$', '$5\\cdot 10^{-2}$']\n",
    "for eps in tols:\n",
    "    print(eps)\n",
    "    ergs  = initializer(a_new_v =x_new[0], b_new_v=x_new[1], n_new_v = n_new,\n",
    "                        a_obs_f =x_obs[0], b_obs_f=x_obs[1], n_obs_f = 6, Î”p =Î”p,\n",
    "                        a_obs_v =x_obs[0], b_obs_v=x_obs[1], va_obs=v_obs[0], vb_obs =v_obs[1], \n",
    "                        fboundary = False, Î´ = 0.00, Ïµ = eps)\n",
    "    couette_epsilon.append(solution(func_SE, dfunc_SE, hess_SE, predictor_SE, np.ones(2), *ergs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eps(ans, args, name):\n",
    "    x , _, xt, _, yt, _ , *_  = args\n",
    "    fig, [ax,bx] = plt.subplots(figsize=(9,9), nrows=2, sharex=True)\n",
    "    clrs = mpl.cm.viridis(np.linspace(0, 0.8, num=len(ans)))\n",
    "    yloc = [0.1, 0.25, 0.4, 0.6, 0.75]\n",
    "    for i, [v,dv,*_] in enumerate(ans):\n",
    "        bx.plot(x, np.abs(vgold - v), color=clrs[i], lw=4)\n",
    "        addtxt(ax,0.85, 0.15+0.17*i, tolbls[i], clr=clrs[i], fs=16)\n",
    "        ax.plot(x, dv, color=clrs[i])\n",
    "        ax.hlines(tols[i], 0, 1, color=clrs[i], ls='--')\n",
    "    addtxt(ax, 0.8, 0.15+0.17*0, '$\\epsilon =$', clr=clrs[0], fs=16)\n",
    "    ax.set_title('Prediction Uncertainty', fontsize=22)\n",
    "    bx.set_title('Absolute Error', fontsize=22)\n",
    "    bx.set_xlabel(r'${x}$', fontsize=22)\n",
    "    ax.semilogy()\n",
    "    bx.semilogy()\n",
    "    bx.set_ylim(1e-12,1e-0)\n",
    "    ax.set_ylim(1e-6,1e-0)\n",
    "    ax.set_xlim(0,1)\n",
    "    fig.tight_layout()\n",
    "    savefig(fig, name, dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eps(couette_epsilon, ergs, 'couette_eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisieulle Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L,Î”p        = 1.0, -1.0,\n",
    "n_obs       = [3, 4, 5, 6, 8, 16, 32, 64]         # number of force training/observation points in [0,L]\n",
    "x_obs,v_obs = np.array([0.0, L]), np.array([0.0, 0.0])\n",
    "\n",
    "n_new = 20\n",
    "x_new = np.array([0.0, L])\n",
    "poiseuille = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poiseuille['se'] = scan_solver('poiseuille_se', func_SE, dfunc_SE, hess_SE, predictor_SE, init=np.array([0, np.log(L/2)]), \n",
    "                               L=L, Î”p=Î”p, n_obs=n_obs, x_obs=x_obs, v_obs=v_obs, n_new=n_new, x_new=x_new, Î´=0.0, Ïµ=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poiseuille['rq'] = scan_solver('poiseuille_rq', func_RQ, dfunc_RQ, hess_RQ, predictor_RQ, init=np.array([0.0, np.log(L/2), 1.0]), \n",
    "                               L=L, Î”p=Î”p, n_obs=n_obs, x_obs=x_obs, v_obs=v_obs, n_new=n_new, x_new=x_new, Î´=0.0, Ïµ=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poiseuille['mt'] = scan_solver('poiseuille_mt', func_MT, dfunc_MT, hess_MT, predictor_MT, init=np.array([-1.0, np.log(L)]), \n",
    "                               L=L, Î”p=Î”p, n_obs=n_obs, x_obs=x_obs, v_obs=v_obs, n_new=n_new, x_new=x_new, Î´=0.0, Ïµ=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "poiseuille['nn'] = scan_solver('poiseuille_nn', func_NN, dfunc_NN, hess_NN, predictor_NN, init=np.array([-1.0, np.log(L)]), \n",
    "                               L=L, Î”p=Î”p, n_obs=n_obs, x_obs=x_obs, v_obs=v_obs, n_new=n_new, x_new=x_new, Î´=0.0, Ïµ=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best results are obtained for Squared-Exponential and Neural-Network\n",
    "- Rational quadratic is ok...\n",
    "- Note the poor convergence of the Matern Kernel in this case, $7/2$ is not bad (not great), $5/2$ is quite bad and $3/2$ never converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poiseuille = {tag : load_solution(f'poiseuille_{tag}') for tag in tags.keys()}\n",
    "vgold      = (lambda x :-0.5/L*x*(x-L))(poiseuille['se']['test']['x'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_solution(poiseuille, 'poiseuille', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error(poiseuille, 'poiseuille_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poiseuille_epsilon = []\n",
    "tols = [5e-6, 5e-5, 5e-4, 5e-3, 5e-2]\n",
    "tolbls = ['$5\\cdot 10^{-6}$', '$5\\cdot 10^{-5}$', '$5\\cdot 10^{-4}$', '$5\\cdot 10^{-3}$', '$5\\cdot 10^{-2}$']\n",
    "for eps in tols:\n",
    "    print(eps)\n",
    "    ergs  = initializer(a_new_v =x_new[0], b_new_v =x_new[1], n_new_v = n_new,\n",
    "                        a_obs_f =x_obs[0], b_obs_f=x_obs[1],  n_obs_f = 6, Î”p =Î”p, \n",
    "                        a_obs_v =x_obs[0], b_obs_v=x_obs[1], va_obs=v_obs[0], vb_obs =v_obs[1],\n",
    "                        fboundary = False, Î´ = 0.00, Ïµ = eps)\n",
    "    poiseuille_epsilon.append(solution(func_SE, dfunc_SE, hess_SE, predictor_SE, np.array([-1.0, np.log(L/4)]), *ergs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_eps(poiseuille_epsilon, ergs, 'poiseuille_eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poiseuille Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisy(name, x_new_v, x_obs_v, x_obs_f, ntest, ntrain, dp, ntrace = 30):\n",
    "    def d2fd(x, f):\n",
    "        dx = x[1]-x[0]\n",
    "        d2f = np.roll(f, -2) + np.roll(f, 2) - 2.0*f\n",
    "        return d2f/(4*dx**2)\n",
    "    key   = random.PRNGKey(0)    \n",
    "    v_obs = np.array([0.0, 0.0])\n",
    "    Î”p=dp*(x_obs_f[1]-x_obs_f[0])\n",
    "    args  = initializer(a_new_v = x_new_v[0], b_new_v = x_new_v[1], n_new_v = ntest,\n",
    "                        a_obs_f = x_obs_f[0], b_obs_f = x_obs_f[1], n_obs_f = ntrain, Î”p = Î”p,\n",
    "                        a_obs_v = x_obs_v[0], b_obs_v = x_obs_v[1], va_obs= v_obs[0], vb_obs=v_obs[1],\n",
    "                        fboundary = False, Î´ = 0.0, Ïµ = 1e-4)\n",
    "    v,dv,opt = solution(func_SE, dfunc_SE, hess_SE, predictor_SE, np.array([0.0, 0.0]), *args)\n",
    "    Î¼,Î£  = predictor_SE(opt['x'], *args)\n",
    "    vrnd = Î¼ + np.einsum('ij,...j->...i', np.linalg.cholesky(Î£ + np.diag(np.ones_like(Î¼)*1e-10)), random.normal(key, (ntrace,) + Î¼.shape))    \n",
    "    \n",
    "    xv, _, _, _, _, xf, *_ = args\n",
    "    lbox  = x_obs_v[1]-x_obs_v[0]\n",
    "    vgold = (lambda x :-0.5/lbox*x*(x-lbox))(xv)\n",
    "\n",
    "    \n",
    "    fig, [ax,bx] = plt.subplots(figsize=(18,12),nrows=2,sharex=True)\n",
    "    ax.fill_between(xv, v-2*dv, v+2*dv, color=color['mid'], lw=4)\n",
    "    ax.plot(xv, v, color=color['dark_highlight'], lw=4.0)\n",
    "    ax.plot(xv, vgold, color=color['superfine'], ls='--', lw=4)\n",
    "    ax.plot(x_obs, v_obs, marker='o', color=color['superfine'], ls='None', ms=16)\n",
    "    ax.plot(xf, np.min(vgold)*np.ones_like(xf), marker='x', mfc='None', ls='None', mew=2, color='k')    \n",
    "\n",
    "    \n",
    "    bx.plot(xf, np.ones_like(xf)*dp, marker='x', mfc='None', ls='None', mew=4, color='k')    \n",
    "    for i in range(ntrace):\n",
    "        ax.plot(xv,vrnd[i],lw=0.5,color=color['dark'])    \n",
    "        fchck= d2fd(xv, vrnd[i])\n",
    "        bx.plot(xv[2:-2], fchck[2:-2], color=color['dark'], lw=0.5)\n",
    "    fchck = d2fd(xv, Î¼)\n",
    "    bx.plot(xv[2:-2], fchck[2:-2], color=color['dark_highlight'], lw=4.0)\n",
    "    bx.plot(xv, np.ones_like(xv)*dp, color=color['superfine'], ls='--',lw=4)\n",
    "        \n",
    "    ax.set_ylabel(r'$v$', fontsize=18)\n",
    "    bx.set_ylabel(r'$\\mathrm{d}^2 v / \\mathrm{d}x^2$', fontsize=18)\n",
    "    bx.set_xlabel(r'$x$', fontsize=18)\n",
    "\n",
    "    fig.tight_layout()    \n",
    "    savefig(fig, name, dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisy('poiseuille_4inside', [-1, 2],[0, 1], [0,1], 96, 4, -1.0, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisy('poiseuille_4inout',[-1, 2],[0, 1], [-1,2], 30, 4, -1.0, 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisy('poiseuille_8inout',[-1, 2],[0, 1], [-1,2], 30, 8, -1.0, 20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
